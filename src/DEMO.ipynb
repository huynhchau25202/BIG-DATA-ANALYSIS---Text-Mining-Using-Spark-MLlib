{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NGHIÊN CỨU ĐIỂN HÌNH-ĐÁNH GIÁ NGÂN HÀNG KHAI THÁC VĂN BẢN / PHÂN TÍCH KHIẾU NẠI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd  # Để làm việc với dữ liệu dạng bảng và chuỗi\n",
    "import numpy as np   # Để xử lý dữ liệu số học và mảng\n",
    "import seaborn as sns  # Để vẽ đồ thị và trực quan hóa dữ liệu\n",
    "import matplotlib.pyplot as plt  # Để vẽ đồ thị\n",
    "\n",
    "import nltk  # Thư viện xử lý ngôn ngữ tự nhiên\n",
    "from nltk.tokenize import word_tokenize  # Để chia văn bản thành các từ\n",
    "from nltk.corpus import stopwords  # Danh sách các từ không cần thiết (\"stop words\")\n",
    "from nltk.stem import WordNetLemmatizer  # Để chuyển các từ về dạng gốc\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import string  # Để làm việc với các phép toán chuỗi cơ bản\n",
    "import re  # Thư viện xử lý biểu thức chính quy (regular expressions)\n",
    "\n",
    "\n",
    "# Download stopwords và wordnet nếu chưa có\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import thư viên máy học spark mlib\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kiểm toán dữ liệu từ tệp CSV dùng pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Stars</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>BankName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4/10/2017</td>\n",
       "      <td>5</td>\n",
       "      <td>Great job, Wyndham Capital! Each person was pr...</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2/10/2017</td>\n",
       "      <td>5</td>\n",
       "      <td>Matthew Richardson is professional and helpful...</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8/21/2017</td>\n",
       "      <td>5</td>\n",
       "      <td>We had a past experience with Wyndham Mortgage...</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12/17/2017</td>\n",
       "      <td>5</td>\n",
       "      <td>We have been dealing with Brad Thomka from the...</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5/27/2016</td>\n",
       "      <td>5</td>\n",
       "      <td>I can't express how grateful I am for the supp...</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Stars                                            Reviews  \\\n",
       "0   4/10/2017      5  Great job, Wyndham Capital! Each person was pr...   \n",
       "1   2/10/2017      5  Matthew Richardson is professional and helpful...   \n",
       "2   8/21/2017      5  We had a past experience with Wyndham Mortgage...   \n",
       "3  12/17/2017      5  We have been dealing with Brad Thomka from the...   \n",
       "4   5/27/2016      5  I can't express how grateful I am for the supp...   \n",
       "\n",
       "                   BankName  \n",
       "0  Wyndham Capital Mortgage  \n",
       "1  Wyndham Capital Mortgage  \n",
       "2  Wyndham Capital Mortgage  \n",
       "3  Wyndham Capital Mortgage  \n",
       "4  Wyndham Capital Mortgage  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dùng pandas import dữ liệu \n",
    "BankReviews=pd.read_csv(\"D:\\\\Niên Luận Cơ Sở_nhóm 2\\\\BankReviews.csv\")\n",
    "\n",
    "#in ra 5 dòng đầu tiên của dữ liệu\n",
    "BankReviews.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in ra kích thước của dữ liệu\n",
    "BankReviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in ra thông tin về dữ liệu\n",
    "BankReviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kiểm tra xem có giá trị thiếu\n",
    "BankReviews.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xử lí Dữ Liệu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loại bỏ cột Data vì nó không dùng\n",
    "banks=BankReviews.drop(['Date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loại bỏ cột ngân hàng\n",
    "banks=banks.drop(['BankName'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chuyển chữ hoa sang thường\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "banks['Reviews_daxuli']=banks['Reviews'].apply(text_lowercase)\n",
    "\n",
    "banks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xóa Dấu câu\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_punct(text):\n",
    "    text_nonpunc=\"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nonpunc\n",
    "\n",
    "banks['Reviews_daxuli']=banks['Reviews_daxuli'].apply(remove_punct)\n",
    "\n",
    "banks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xóa Khoảng Trắng\n",
    "def remove_whitespaces(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "banks['Reviews_daxuli']=banks['Reviews_daxuli'].apply(lambda x:remove_whitespaces(x))\n",
    "\n",
    "banks.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MÃ HÓA DỮ LIỆU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tách Từ bằng dấu ,\n",
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = re.split(r'\\W', text)\n",
    "    return tokens\n",
    "\n",
    "banks['Reviews_daxuli'] = banks['Reviews_daxuli'].apply(lambda x: tokenize(x))\n",
    "\n",
    "banks.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xóa stopwords\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "nltk.download('vader_lexicon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_list):\n",
    "    filtered_text=[word for word in tokenized_list if word not in stop]\n",
    "    return filtered_text\n",
    "\n",
    "banks['Reviews_daxuli'] = banks['Reviews_daxuli'].apply(lambda x:remove_stopwords(x))\n",
    "\n",
    "banks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization kểu nhưng chuyển những từ run, ran, running, runs về từ góc là run\n",
    "\n",
    "wn=nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizing(text):\n",
    "    lemma=[wn.lemmatize(word) for word in text]\n",
    "    return lemma\n",
    "\n",
    "banks['Reviews_daxuli'] = banks['Reviews_daxuli'].apply(lemmatizing)\n",
    "\n",
    "banks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Đếm Từ thường gặp\n",
    "word_count={}\n",
    "\n",
    "for sentence in banks['Reviews_daxuli']:\n",
    "    for word in sentence:\n",
    "        \n",
    "        if word not in word_count:\n",
    "            word_count[word]=1\n",
    "            \n",
    "        else:\n",
    "            word_count[word]+=1  \n",
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chuyển đổi word_count thành dataframe\n",
    "\n",
    "df=pd.DataFrame(word_count.items(), columns=['word','word_count'])\n",
    "df=df.sort_values('word_count', ascending=False).reset_index(drop=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PHÂN TÍCH TÌNH CẢM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mức độ tích cực\n",
    "def sentiment_pos(sentence):\n",
    "    sent=analyser.polarity_scores(sentence)\n",
    "    return sent['pos']\n",
    "\n",
    "# mức độ tiêu cực\n",
    "def sentiment_neg(sentence):\n",
    "    sent=analyser.polarity_scores(sentence)\n",
    "    return sent['neg']\n",
    "\n",
    "#mức độ trung lập\n",
    "def sentiment_neu(sentence):\n",
    "    sent=analyser.polarity_scores(sentence)\n",
    "    return sent['neu']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "#tính mức tích cực\n",
    "df['positive']=df['word'].apply(sentiment_pos)\n",
    "\n",
    "#tính mức tiêu cực\n",
    "df['negative']=df['word'].apply(sentiment_neg)\n",
    "\n",
    "#tính mức độ trung lập (TÍNH TRUNG LẬP NÀY KHÔNG CẦN THIẾT CÓ THỂ LOẠI BỎ)\n",
    "df['neutral'] = df['word'].apply(sentiment_neu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tìm những từ tích cực chính\n",
    "df1=df.loc[df.positive>0]['word']\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tìm những từ tiêu cực chính\n",
    "df2=df.loc[df.negative>0]['word']\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TÍNH TRUNG LẬP NÀY KHÔNG CẦN THIẾT CÓ THỂ LOẠI BỎ\n",
    "df3 = df.loc[df['neutral'] > 0]['word']\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PHÂN LOẠI ĐÁNH GIÁ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banks['positive']=banks['Reviews'].apply(sentiment_pos)\n",
    "banks['negative']=banks['Reviews'].apply(sentiment_neg)\n",
    "banks['neutral']=banks['Reviews'].apply(sentiment_neu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_words=''\n",
    "stopwords=set(STOPWORDS)\n",
    "\n",
    "for x in banks.Reviews: #Lặp qua từng đánh giá\n",
    "    \n",
    "    x=str(x) # Chuyển đổi đánh giá thành chuỗi\n",
    "    \n",
    "    tokens=x.split() #Tách các từ trong đánh giá\n",
    "    \n",
    "    for i in range(len(tokens)): # Lặp qua từng từ \n",
    "        tokens[i]=tokens[i].lower() #huyển đổi các từ thành chữ thường\n",
    "        \n",
    "    for y in tokens: #Lặp qua từng từ trong danh sách các từ\n",
    "        comment_words=comment_words+y+' '#Thêm từ vào chuỗi 'comment_words' cùng với một khoảng trắng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud=WordCloud(width = 800, height = 800, \n",
    "                background_color ='black', \n",
    "                stopwords = stopwords, \n",
    "                min_font_size = 10).generate(comment_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XÁC ĐỊNH CÁC CHỦ ĐỀ CHÍNH CỦA VẤN ĐỀ BẰNG THUẬT TOÁN K-MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sử Dụng Thuật Toán Để Gom Cụm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Khởi tạo SparkSession\n",
    "spark = SparkSession.builder.appName(\"KMeansExample\").getOrCreate()\n",
    "\n",
    "\n",
    "# Định nghĩa các hàm UDF\n",
    "sentiment_pos_udf = udf(sentiment_pos, FloatType())\n",
    "sentiment_neg_udf = udf(sentiment_neg, FloatType())\n",
    "sentiment_neu_udf = udf(sentiment_neu, FloatType())\n",
    "\n",
    "# Thêm các cột positive, negative, neutral vào DataFrame banks\n",
    "banks['positive'] = banks['Reviews'].apply(sentiment_pos)\n",
    "banks['negative'] = banks['Reviews'].apply(sentiment_neg)\n",
    "banks['neutral'] = banks['Reviews'].apply(sentiment_neu)\n",
    "\n",
    "# Chuyển đổi DataFrame pandas thành DataFrame Spark\n",
    "banks_spark = spark.createDataFrame(banks)\n",
    "\n",
    "# Lấy các đánh giá tiêu cực để xác định các vấn đề chính\n",
    "problems = banks_spark.filter(banks_spark['negative'] > 0).select('Reviews')\n",
    "\n",
    "# Tokenize các đánh giá\n",
    "tokenizer = Tokenizer(inputCol=\"Reviews\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(problems)\n",
    "\n",
    "# Tạo vector đặc trưng sử dụng HashingTF\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "# Tính toán IDF cho các vector đặc trưng\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "# Áp dụng thuật toán KMeans để phân cụm\n",
    "kmeans = KMeans(k=3, seed=42)\n",
    "model = kmeans.fit(rescaledData)\n",
    "\n",
    "# Dự đoán phân cụm cho dữ liệu\n",
    "predictions = model.transform(rescaledData)\n",
    "\n",
    "# Hiển thị kết quả\n",
    "predictions.select(\"Reviews\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tiếp tục với các bước xử lý kết quả phân cụm\n",
    "# thống kê số lượng điểm dữ liệu trong mỗi phân cụm\n",
    "cluster_counts = predictions.groupBy(\"prediction\").count()\n",
    "cluster_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tạo DataFrame từ danh sách các văn bản\n",
    "df = spark.createDataFrame([(1, \"Reviews\"), (2, \"Reviews_daxuli\"), (3, \"positive\"), (4, \"negative\"),(5, \"neutral\")], [\"id\", \"text\"])\n",
    "\n",
    "# Tạo các bước xử lý dữ liệu văn bản trong pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=20)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "# Tạo pipeline và thực thi các bước xử lý dữ liệu\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwords_remover, hashingTF, idf])\n",
    "pipeline_model = pipeline.fit(df)\n",
    "df_transformed = pipeline_model.transform(df)\n",
    "\n",
    "# Hiển thị kết quả\n",
    "df_transformed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VECTOR HÓA DỮ LIỆU THÔ: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Tạo VectorAssembler để kết hợp các cột features thành một cột vector\n",
    "assembler = VectorAssembler(inputCols=[\"features\"], outputCol=\"vector_features\")\n",
    "df_transformed = assembler.transform(df_transformed)\n",
    "\n",
    "# Lấy dữ liệu vector hóa X từ DataFrame\n",
    "X = df_transformed.select(\"vector_features\").rdd.map(lambda x: x[0]).collect()\n",
    "\n",
    "# Số cụm\n",
    "k = 3\n",
    "\n",
    "# Tạo mô hình KMeans\n",
    "kmeans = KMeans(k=k, seed=123)\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "model = kmeans.fit(df_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, Tokenizer, Word2Vec\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Chuyển đổi cột \"text\" thành vector số\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"1text_token\")\n",
    "df_transformed = tokenizer.transform(df_transformed)\n",
    "\n",
    "# Điều chỉnh giá trị minCount\n",
    "word2vec = Word2Vec(inputCol=\"1text_token\", outputCol=\"1text_features\", minCount=1)\n",
    "model = word2vec.fit(df_transformed)\n",
    "df_transformed = model.transform(df_transformed)\n",
    "\n",
    "# Chuyển đổi cột \"words\" thành vector số\n",
    "word2vec = Word2Vec(inputCol=\"words\", outputCol=\"1words_features\", minCount=1)\n",
    "model = word2vec.fit(df_transformed)\n",
    "df_transformed = model.transform(df_transformed)\n",
    "\n",
    "# Tạo VectorAssembler để kết hợp các cột features thành một cột vector\n",
    "assembler = VectorAssembler(inputCols=[\"1text_features\", \"1words_features\"], outputCol=\"1vector_features\")\n",
    "df_transformed = assembler.transform(df_transformed)\n",
    "\n",
    "# Số cụm\n",
    "k = 3\n",
    "\n",
    "# Tạo mô hình KMeans\n",
    "kmeans = KMeans(k=k, seed=123)\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "model = kmeans.fit(df_transformed)\n",
    "\n",
    "# Lấy kết quả cluster từ mô hình\n",
    "predictions = model.transform(df_transformed)\n",
    "\n",
    "# Tính toán kích thước của các cụm\n",
    "cluster_sizes = predictions.groupBy(\"prediction\").count().collect()\n",
    "cluster_sizes = [row[\"count\"] for row in cluster_sizes]\n",
    "print(\"Cluster sizes:\", cluster_sizes)\n",
    "\n",
    "# Truy cập vào trung tâm cụm\n",
    "centroids = model.clusterCenters()\n",
    "\n",
    "# In kết quả\n",
    "print(\"Cluster centers:\")\n",
    "for i, centroid in enumerate(centroids):\n",
    "    print(\"Cluster\", i, \"mean:\", centroid.mean())\n",
    "    print(\"Cluster\", i, \"std:\", centroid.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Số cụm\n",
    "k = 3\n",
    "\n",
    "# Tạo mô hình KMeans\n",
    "kmeans = KMeans(k=k, seed=123)\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "model = kmeans.fit(df_transformed)\n",
    "\n",
    "# Truy cập vào trung tâm cụm\n",
    "centroids = model.clusterCenters()\n",
    "\n",
    "# In kết quả\n",
    "for i in range(0, k):\n",
    "    print('\\n\\nCluster %d:' % i)\n",
    "    for ind in centroids[i][:10]:\n",
    "        print('\\t  - %s' % ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "\n",
    "# Chuyển đổi DataFrame Pandas 'banks' sang Spark DataFrame\n",
    "spark_df = spark.createDataFrame(banks)\n",
    "\n",
    "# Sử dụng Tokenizer để tách từ\n",
    "tokenizer = Tokenizer(inputCol=\"Reviews\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(spark_df)\n",
    "\n",
    "# Sử dụng HashingTF để tạo các tính năng từ từ vựng\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=2000)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "# Sử dụng IDF để tính toán trọng số IDF cho các tính năng\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "transformed_banks = idfModel.transform(featurizedData)\n",
    "\n",
    "# Huấn luyện mô hình KMeans\n",
    "kmeans = KMeans(k=3, seed=42)\n",
    "model = kmeans.fit(transformed_banks)\n",
    "\n",
    "# Sử dụng mô hình KMeans đã huấn luyện để dự đoán cụm\n",
    "predictions = model.transform(transformed_banks)\n",
    "\n",
    "# Thêm cột \"cluster\" vào DataFrame \"predictions\" từ cột \"prediction\"\n",
    "predictions_with_cluster = predictions.withColumn(\"cluster\", col(\"prediction\"))\n",
    "\n",
    "# Hiển thị một số dòng đầu tiên của DataFrame \"predictions_with_cluster\"\n",
    "predictions_with_cluster.show()\n",
    "\n",
    "# Gán cụm dự đoán vào một cột mới 'cluster' trong DataFrame\n",
    "banks_with_clusters = predictions.withColumn(\"cluster\", col(\"prediction\"))\n",
    "\n",
    "# Hiển thị một số dòng đầu tiên của DataFrame 'banks_with_clusters'\n",
    "banks_with_clusters.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KỸ THUẬT HỒI QUY LOGISTIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Tạo SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Tạo DataFrame từ danh sách các từ điển banks\n",
    "data = spark.createDataFrame(banks, [\"label\"])\n",
    "\n",
    "# Lọc dữ liệu và loại bỏ các hàng chứa giá trị null trong cột \"label\"\n",
    "data = data.filter(col(\"label\").isNotNull())\n",
    "\n",
    "# Chuyển đổi kiểu dữ liệu của cột \"label\" thành số nguyên (integer)\n",
    "data = data.withColumn(\"label\", col(\"label\").cast(\"integer\"))\n",
    "\n",
    "# Tạo cột features từ các cột khác trong DataFrame\n",
    "assembler = VectorAssembler(inputCols=[\"label\"], outputCol=\"features\")\n",
    "data = assembler.transform(data)\n",
    "\n",
    "# Chia dữ liệu thành tập huấn luyện và tập kiểm tra\n",
    "train_data, test_data = data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Tạo mô hình Logistic Regression\n",
    "logistic_regression = LogisticRegression()\n",
    "\n",
    "# Huấn luyện mô hình trên tập huấn luyện\n",
    "model = logistic_regression.fit(train_data)\n",
    "\n",
    "# Dự đoán trên tập huấn luyện và tập kiểm tra\n",
    "train_predictions = model.transform(train_data)\n",
    "test_predictions = model.transform(test_data)\n",
    "\n",
    "# Đánh giá độ chính xác trên tập huấn luyện và tập kiểm tra\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "train_accuracy = evaluator.evaluate(train_predictions)\n",
    "test_accuracy = evaluator.evaluate(test_predictions)\n",
    "\n",
    "# In kết quả\n",
    "print(\"train_accuracy:\", train_accuracy)\n",
    "print(\"test_accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType, ArrayType, FloatType\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Định nghĩa hàm UDF để chuyển đổi cột dự đoán thành danh sách các xác suất\n",
    "udf_to_list = udf(lambda x: [float(i) for i in x], ArrayType(FloatType()))\n",
    "train_predictions = train_predictions.withColumn(\"probability_list\", udf_to_list(\"probability\"))\n",
    "test_predictions = test_predictions.withColumn(\"probability_list\", udf_to_list(\"probability\"))\n",
    "\n",
    "# Chuyển đổi DataFrame thành Pandas DataFrame\n",
    "train_predictions_pd = train_predictions.select(\"label\", \"probability_list\").toPandas()\n",
    "test_predictions_pd = test_predictions.select(\"label\", \"probability_list\").toPandas()\n",
    "\n",
    "# Xác định số lượng lớp\n",
    "num_classes = train_predictions_pd[\"label\"].nunique()\n",
    "\n",
    "# Kiểm tra số lượng lớp\n",
    "if num_classes < 2:\n",
    "    roc_auc_scores = [0]  # Trả về giá trị mặc định khi chỉ có một lớp\n",
    "else:\n",
    "    # Tính roc_auc_score cho từng lớp\n",
    "    roc_auc_scores = []\n",
    "    for class_label in range(num_classes):\n",
    "        y_true = (train_predictions_pd[\"label\"] == class_label).astype(int)\n",
    "        y_score = train_predictions_pd[\"probability_list\"].apply(lambda x: x[class_label])\n",
    "\n",
    "        # Kiểm tra số lượng lớp trong y_true\n",
    "        if y_true.sum() > 1:\n",
    "            roc_auc = roc_auc_score(y_true, y_score)\n",
    "        else:\n",
    "            roc_auc = 0  # Trả về giá trị mặc định khi chỉ có một lớp\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "\n",
    "# In kết quả roc_auc_score cho từng lớp\n",
    "for class_label, roc_auc in enumerate(roc_auc_scores):\n",
    "    print(f\"Roc Auc Score of class {class_label}: {roc_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Chuyển DataFrame của bạn thành một Temporary View\n",
    "test_predictions.createOrReplaceTempView(\"test_predictions\")\n",
    "\n",
    "# Sử dụng SQL query để tính toán ma trận nhầm lẫn\n",
    "confusion_matrix = spark.sql(\"\"\"\n",
    "    SELECT label, prediction, count(*) as count\n",
    "    FROM test_predictions\n",
    "    GROUP BY label, prediction\n",
    "    ORDER BY label, prediction\n",
    "\"\"\")\n",
    "\n",
    "# Hiển thị ma trận nhầm lẫn\n",
    "confusion_matrix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Chuyển DataFrame của bạn thành một Temporary View\n",
    "test_predictions.createOrReplaceTempView(\"test_predictions\")\n",
    "\n",
    "# Sử dụng SQL query để tính toán ma trận nhầm lẫn\n",
    "confusion_matrix = spark.sql(\"\"\"\n",
    "    SELECT label, prediction, count(*) as count\n",
    "    FROM test_predictions\n",
    "    GROUP BY label, prediction\n",
    "    ORDER BY label, prediction\n",
    "\"\"\")\n",
    "\n",
    "# Chuyển đổi DataFrame thành Pandas DataFrame\n",
    "confusion_matrix_pd = confusion_matrix.toPandas()\n",
    "\n",
    "# Tạo ma trận nhầm lẫn bằng cách sử dụng cột \"count\" trong Pandas DataFrame\n",
    "cm = confusion_matrix_pd.pivot(index='label', columns='prediction', values='count')\n",
    "\n",
    "# Hiển thị ma trận nhầm lẫn bằng Heatmap\n",
    "plt.imshow(cm, cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.xticks([0, 1], [\"1\", \"5\"])\n",
    "plt.yticks([0, 1], [\"1\", \"5\"])\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Thêm cột predicted_stars vào DataFrame banks_with_clusters\n",
    "final_df = banks_with_clusters.withColumn(\"predicted_stars\", lit(banks_with_clusters[\"Stars\"]))\n",
    "\n",
    "final_df = final_df.drop(\"words\", \"rawFeatures\", \"features\", \"prediction\",\"cluster\")\n",
    "\n",
    "# Hiển thị bảng cuối cùng final_df\n",
    "final_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
